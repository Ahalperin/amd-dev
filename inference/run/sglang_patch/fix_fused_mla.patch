diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py
index 2b1c3c04d..b143ba99a 100644
--- a/python/sglang/srt/models/deepseek_v2.py
+++ b/python/sglang/srt/models/deepseek_v2.py
@@ -2191,10 +2191,6 @@ class DeepseekV2AttentionMLA(nn.Module):
         enable_rope_fusion = (
             os.getenv("SGLANG_FUSED_MLA_ENABLE_ROPE_FUSION", "1") == "1"
         )
-        q_len = hidden_states.shape[0]
-        q_input = hidden_states.new_empty(
-            q_len, self.num_local_heads, self.kv_lora_rank + self.qk_rope_head_dim
-        )
         if self.q_lora_rank is not None:
             q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(
                 [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1
@@ -2208,6 +2204,11 @@ class DeepseekV2AttentionMLA(nn.Module):
             latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
         q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
 
+        q_len = q.shape[0]
+        q_input = q.new_empty(
+            q_len, self.num_local_heads, self.kv_lora_rank + self.qk_rope_head_dim
+        )
+
         if _is_hip:
             # TODO(haishaw): add bmm_fp8 to ROCm
             q_nope_out = torch.bmm(
@@ -2249,9 +2250,16 @@ class DeepseekV2AttentionMLA(nn.Module):
             dtype=q.dtype,
             device=q.device,
         )
-        attn_logits, _, kv_indptr, kv_indices, _, _, _ = (
-            forward_batch.attn_backend.forward_metadata
-        )
+        if isinstance(forward_batch.attn_backend.forward_metadata, tuple):
+            attn_logits, _, kv_indptr, kv_indices, _, _, _ = (
+                forward_batch.attn_backend.forward_metadata
+            )
+        else:
+            # Support dataclass ForwardMetadata
+            attn_logits = getattr(forward_batch.attn_backend.forward_metadata, "attn_logits", None)
+            kv_indptr = getattr(forward_batch.attn_backend.forward_metadata, "kv_indptr", None)
+            kv_indices = getattr(forward_batch.attn_backend.forward_metadata, "kv_indices", None)
+            
         cos_sin_cache = self.rotary_emb.cos_sin_cache
         num_kv_split = forward_batch.attn_backend.num_kv_splits
         sm_scale = self.attn_mqa.scaling
